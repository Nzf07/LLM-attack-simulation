{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFbzmoTNeJvsxCWiWTFWGT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Denial of Service (DoS) Injections Lab\n",
        "\n",
        "This Notebook will simulate a DoS attack on GPT-2 using concurrent threading rendering the model unavailable for upto 30 seconds."
      ],
      "metadata": {
        "id": "4aVP4HSzOPH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expLv18b6FRH"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import concurrent.futures\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "UUnm-46U6K9X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "rdvAth9Q6O5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "    return generator(prompt, max_length=50)[0]['generated_text']"
      ],
      "metadata": {
        "id": "yG7YRJ3B6Rqu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_request(prompt):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        result = generate_text(prompt)\n",
        "        end_time = time.time()\n",
        "        print(f\"Generated text: {result[:50]}... (Time taken: {end_time - start_time:.2f} seconds)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")"
      ],
      "metadata": {
        "id": "ALxO5kSn6SoF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_dos_attack(prompt, num_requests):\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = [executor.submit(simulate_request, prompt) for _ in range(num_requests)]\n",
        "        concurrent.futures.wait(futures)\n"
      ],
      "metadata": {
        "id": "zTBMt-UE6VY9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    prompt = \"Once upon a time\"\n",
        "    num_requests = 10  # Number of parallel requests to simulate DoS\n",
        "\n",
        "    print(f\"Simulating Denial of Service attack with {num_requests} requests...\")\n",
        "    simulate_dos_attack(prompt, num_requests)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9fc0lXU6W9e",
        "outputId": "daeea28f-230c-44a4-93c9-ebfef0b6891d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating Denial of Service attack with 10 requests...\n",
            "Generated text: Once upon a time, everyone felt like they'd done s... (Time taken: 28.98 seconds)\n",
            "Generated text: Once upon a time, they were not all that different... (Time taken: 29.03 seconds)\n",
            "Generated text: Once upon a time, everyone who was willing had to ... (Time taken: 29.07 seconds)\n",
            "Generated text: Once upon a time, when you could hear him from the... (Time taken: 29.22 seconds)\n",
            "Generated text: Once upon a time you get your head around the idea... (Time taken: 29.25 seconds)\n",
            "Generated text: Once upon a time, the moonlight will always shine ... (Time taken: 29.41 seconds)\n",
            "Generated text: Once upon a time, his father and uncle were called... (Time taken: 29.36 seconds)\n",
            "Generated text: Once upon a time there was a time when you didn't ... (Time taken: 29.43 seconds)\n",
            "Generated text: Once upon a time, the entire world had ceased to e... (Time taken: 29.45 seconds)\n",
            "Generated text: Once upon a time, my life was quite filled with ha... (Time taken: 29.57 seconds)\n"
          ]
        }
      ]
    }
  ]
}